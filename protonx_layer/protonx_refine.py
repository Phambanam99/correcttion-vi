import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

MODEL_NAME = "protonx-models/protonx-legal-tc"

# === LOG: Device Info ===
device = "cpu"  # ProtonX cháº¡y trÃªn CPU
print(f"ğŸ–¥ï¸  [ProtonX] Device: {device.upper()}")
print(f"ğŸ–¥ï¸  [ProtonX] Model: {MODEL_NAME}")
print("=" * 50)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(
    MODEL_NAME,
    device_map="cpu",      # Ráº¤T QUAN TRá»ŒNG
    torch_dtype=torch.float32
)

model.eval()

def refine_text(text: str) -> str:
    # === LOG: ProtonX Input ===
    print("\n" + "=" * 50)
    print("ğŸ“¥ [ProtonX] INPUT:")
    print("-" * 50)
    print(text)
    print("-" * 50)

    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512
    )

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            num_beams=4,
            early_stopping=True
        )

    result = tokenizer.decode(
        outputs[0],
        skip_special_tokens=True
    )

    # === LOG: ProtonX Output ===
    print("ğŸ“¤ [ProtonX] OUTPUT:")
    print("-" * 50)
    print(result)
    print("=" * 50)

    return result


def refine_text_chunked(text: str, max_words_per_chunk: int = 100) -> str:
    """
    Refine vÄƒn báº£n dÃ i báº±ng cÃ¡ch chia thÃ nh chunks theo CÃ‚U.
    Äáº£m báº£o khÃ´ng cáº¯t giá»¯a cÃ¢u.
    """
    import re
    
    words = text.split()
    
    if len(words) <= max_words_per_chunk:
        return refine_text(text)
    
    # Chia theo cÃ¢u (dáº¥u . ! ? káº¿t thÃºc)
    sentences = re.split(r'(?<=[.!?])\s+', text)
    
    # Gom cÃ¢u thÃ nh chunks, má»—i chunk khÃ´ng quÃ¡ max_words_per_chunk tá»«
    chunks = []
    current_chunk = []
    current_word_count = 0
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        sentence_word_count = len(sentence.split())
        
        # Náº¿u 1 cÃ¢u Ä‘Ã£ quÃ¡ dÃ i â†’ xá»­ lÃ½ riÃªng
        if sentence_word_count > max_words_per_chunk:
            # LÆ°u chunk hiá»‡n táº¡i trÆ°á»›c
            if current_chunk:
                chunks.append(" ".join(current_chunk))
                current_chunk = []
                current_word_count = 0
            # ThÃªm cÃ¢u dÃ i nhÆ° 1 chunk riÃªng
            chunks.append(sentence)
        # Náº¿u thÃªm cÃ¢u nÃ y váº«n trong giá»›i háº¡n
        elif current_word_count + sentence_word_count <= max_words_per_chunk:
            current_chunk.append(sentence)
            current_word_count += sentence_word_count
        # Náº¿u thÃªm cÃ¢u nÃ y vÆ°á»£t giá»›i háº¡n â†’ táº¡o chunk má»›i
        else:
            if current_chunk:
                chunks.append(" ".join(current_chunk))
            current_chunk = [sentence]
            current_word_count = sentence_word_count
    
    # ThÃªm chunk cuá»‘i cÃ¹ng
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    
    print(f"ğŸ“¦ [ProtonX] Chia thÃ nh {len(chunks)} chunks (theo cÃ¢u, max {max_words_per_chunk} tá»«/chunk)")
    
    # Xá»­ lÃ½ tá»«ng chunk
    refined_chunks = []
    for idx, chunk in enumerate(chunks, 1):
        print(f"  ğŸ”· ProtonX Chunk [{idx}/{len(chunks)}]: {len(chunk.split())} tá»«")
        refined_chunk = refine_text(chunk)
        refined_chunks.append(refined_chunk)
    
    # GhÃ©p láº¡i
    result = " ".join(refined_chunks)
    
    return result

