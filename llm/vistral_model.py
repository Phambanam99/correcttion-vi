# -*- coding: utf-8 -*-
"""
Vistral-7B-Chat Model
Model ti·∫øng Vi·ªát d·ª±a tr√™n Mistral, fine-tuned cho chat

NOTE: ƒê√¢y l√† gated model, c·∫ßn ƒëƒÉng nh·∫≠p HuggingFace.
C√°ch 1: Set bi·∫øn m√¥i tr∆∞·ªùng HF_TOKEN
C√°ch 2: Ch·∫°y `huggingface-cli login` trong terminal
"""

import torch
import re
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login
from llm.prompts import SYSTEM_PROMPT

MODEL_NAME = "Viet-Mistral/Vistral-7B-Chat"

# === HuggingFace Login ===
HF_TOKEN = os.environ.get("HF_TOKEN", None)
if HF_TOKEN:
    print("üîë [Vistral] ƒêang ƒëƒÉng nh·∫≠p HuggingFace...")
    login(token=HF_TOKEN)
    print("‚úÖ [Vistral] ƒêƒÉng nh·∫≠p th√†nh c√¥ng!")
else:
    print("‚ö†Ô∏è [Vistral] Kh√¥ng t√¨m th·∫•y HF_TOKEN. Th·ª≠ login t·ª´ cache...")

# === LOG: Device Info ===
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üñ•Ô∏è  [Vistral] Device: {device.upper()}")
if device == "cuda":
    print(f"üñ•Ô∏è  [Vistral] GPU: {torch.cuda.get_device_name(0)}")
print(f"üñ•Ô∏è  [Vistral] Model: {MODEL_NAME}")
print("=" * 50)

# Load tokenizer v√† model
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, token=HF_TOKEN)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    load_in_4bit=True,
    torch_dtype=torch.float16,
    trust_remote_code=True,
    token=HF_TOKEN
)


def correct_text(text: str) -> tuple[str, str]:
    """
    S·ª≠a l·ªói vƒÉn b·∫£n ti·∫øng Vi·ªát b·∫±ng Vistral.
    Tr·∫£ v·ªÅ tuple (vƒÉn_b·∫£n_ƒë√£_s·ª≠a, gi·∫£i_th√≠ch).
    """
    # Format theo Mistral chat template
    prompt = f"""<s>[INST] {SYSTEM_PROMPT}

ƒêo·∫°n vƒÉn g·ªëc:
{text}

Tr·∫£ l·ªùi theo format (CH·ªà 1 L·∫¶N, KH√îNG l·∫∑p l·∫°i):
[VƒÇN B·∫¢N ƒê√É S·ª¨A]
(vi·∫øt ƒëo·∫°n vƒÉn ƒë√£ s·ª≠a ·ªü ƒë√¢y)

[GI·∫¢I TH√çCH]
(li·ªát k√™ c√°c thay ƒë·ªïi ·ªü ƒë√¢y m·ªôt c√°ch ng·∫Øn g·ªçn nh·∫•t)

B·∫Øt ƒë·∫ßu:
[VƒÇN B·∫¢N ƒê√É S·ª¨A]
[/INST]"""

    # === LOG: Vistral Input ===
    print("\n" + "=" * 50)
    print("üì• [Vistral] INPUT:")
    print("-" * 50)
    print(text[:200] + "..." if len(text) > 200 else text)
    print("-" * 50)

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.eos_token_id
        )

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Parse k·∫øt qu·∫£ ƒë·ªÉ t√°ch vƒÉn b·∫£n v√† gi·∫£i th√≠ch
    corrected_text = ""
    explanation = ""
    
    # T√¨m T·∫§T C·∫¢ c√°c ph·∫ßn [VƒÇN B·∫¢N ƒê√É S·ª¨A] v√† l·∫•y ph·∫ßn CU·ªêI C√ôNG
    all_matches = list(re.finditer(
        r'\[VƒÇN B·∫¢N ƒê√É S·ª¨A\]\s*(.*?)(?=\[GI·∫¢I TH[I√ç·ªä][√ä·∫æ·ªÜ]?[CT]H?\]|\[VƒÇN B·∫¢N|```|$)', 
        result, re.DOTALL | re.IGNORECASE
    ))
    if all_matches:
        corrected_text = all_matches[-1].group(1).strip()
    else:
        # Fallback: l·∫•y ph·∫ßn sau [/INST]
        parts = result.split("[/INST]")
        if len(parts) > 1:
            corrected_text = parts[-1].strip()
        else:
            corrected_text = text  # Gi·ªØ nguy√™n n·∫øu kh√¥ng parse ƒë∆∞·ª£c
    
    # T√¨m ph·∫ßn [GI·∫¢I TH√çCH]
    explain_match = re.search(r'\[GI·∫¢I TH[I√ç·ªä][√ä·∫æ·ªÜ]?[CT]H?\]\s*(.*?)$', result, re.DOTALL | re.IGNORECASE)
    if explain_match:
        explanation = explain_match.group(1).strip()
    
    # L√†m s·∫°ch vƒÉn b·∫£n
    corrected_text = re.sub(r'```.*?```', '', corrected_text, flags=re.DOTALL)
    corrected_text = re.sub(r'\[GI·∫¢I TH.*', '', corrected_text, flags=re.DOTALL | re.IGNORECASE)
    corrected_text = corrected_text.strip('` \n\t')
    
    # === LOG: Vistral Output ===
    print("üì§ [Vistral] OUTPUT:")
    print("-" * 50)
    print(f"VƒÉn b·∫£n: {corrected_text[:100]}..." if len(corrected_text) > 100 else f"VƒÉn b·∫£n: {corrected_text}")
    print(f"Gi·∫£i th√≠ch: {explanation[:100]}..." if explanation and len(explanation) > 100 else f"Gi·∫£i th√≠ch: {explanation}" if explanation else "Kh√¥ng c√≥ gi·∫£i th√≠ch")
    print("=" * 50)

    return corrected_text, explanation
