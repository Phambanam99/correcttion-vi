# -*- coding: utf-8 -*-
"""
BartPho Autocorrect Model
Model Ä‘Æ°á»£c fine-tune Ä‘áº·c biá»‡t cho sá»­a lá»—i chÃ­nh táº£ tiáº¿ng Viá»‡t
"""

import torch
from transformers import AutoTokenizer, MBartForConditionalGeneration

MODEL_NAME = "bmd1905/vietnamese-correction-v2"

# === LOG: Device Info ===
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ–¥ï¸  [BartPho] Device: {device.upper()}")
if device == "cuda":
    print(f"ğŸ–¥ï¸  [BartPho] GPU: {torch.cuda.get_device_name(0)}")
print(f"ğŸ–¥ï¸  [BartPho] Model: {MODEL_NAME}")
print("=" * 50)

# Load tokenizer vÃ  model
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = MBartForConditionalGeneration.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32
)
model = model.to(device)
model.eval()


def correct_text(text: str) -> str:
    """
    Sá»­a lá»—i chÃ­nh táº£ tiáº¿ng Viá»‡t báº±ng BartPho.
    Tráº£ vá» vÄƒn báº£n Ä‘Ã£ sá»­a.
    """
    # === LOG: BartPho Input ===
    print("\n" + "=" * 50)
    print("ğŸ“¥ [BartPho] INPUT:")
    print("-" * 50)
    print(text[:200] + "..." if len(text) > 200 else text)
    print("-" * 50)

    # Tokenize
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512,
        padding=True
    ).to(device)

    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            num_beams=4,
            early_stopping=True,
            no_repeat_ngram_size=3
        )

    # Decode
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # === LOG: BartPho Output ===
    print("ğŸ“¤ [BartPho] OUTPUT:")
    print("-" * 50)
    print(result[:200] + "..." if len(result) > 200 else result)
    print("=" * 50)

    return result


def correct_text_chunked(text: str, max_words_per_chunk: int = 100) -> str:
    """
    Sá»­a lá»—i vÄƒn báº£n dÃ i báº±ng cÃ¡ch chia thÃ nh chunks theo CÃ‚U.
    Äáº£m báº£o khÃ´ng cáº¯t giá»¯a cÃ¢u.
    """
    import re
    
    words = text.split()
    
    if len(words) <= max_words_per_chunk:
        return correct_text(text)
    
    # Chia theo cÃ¢u (dáº¥u . ! ? káº¿t thÃºc)
    sentences = re.split(r'(?<=[.!?])\s+', text)
    
    # Gom cÃ¢u thÃ nh chunks, má»—i chunk khÃ´ng quÃ¡ max_words_per_chunk tá»«
    chunks = []
    current_chunk = []
    current_word_count = 0
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        sentence_word_count = len(sentence.split())
        
        # Náº¿u 1 cÃ¢u Ä‘Ã£ quÃ¡ dÃ i â†’ xá»­ lÃ½ riÃªng
        if sentence_word_count > max_words_per_chunk:
            # LÆ°u chunk hiá»‡n táº¡i trÆ°á»›c
            if current_chunk:
                chunks.append(" ".join(current_chunk))
                current_chunk = []
                current_word_count = 0
            # ThÃªm cÃ¢u dÃ i nhÆ° 1 chunk riÃªng
            chunks.append(sentence)
        # Náº¿u thÃªm cÃ¢u nÃ y váº«n trong giá»›i háº¡n
        elif current_word_count + sentence_word_count <= max_words_per_chunk:
            current_chunk.append(sentence)
            current_word_count += sentence_word_count
        # Náº¿u thÃªm cÃ¢u nÃ y vÆ°á»£t giá»›i háº¡n â†’ táº¡o chunk má»›i
        else:
            if current_chunk:
                chunks.append(" ".join(current_chunk))
            current_chunk = [sentence]
            current_word_count = sentence_word_count
    
    # ThÃªm chunk cuá»‘i cÃ¹ng
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    
    print(f"ğŸ“¦ [BartPho] Chia thÃ nh {len(chunks)} chunks (theo cÃ¢u, max {max_words_per_chunk} tá»«/chunk)")
    
    # Xá»­ lÃ½ tá»«ng chunk
    corrected_chunks = []
    for idx, chunk in enumerate(chunks, 1):
        print(f"  ğŸ”· Chunk [{idx}/{len(chunks)}]: {len(chunk.split())} tá»«")
        corrected_chunk = correct_text(chunk)
        corrected_chunks.append(corrected_chunk)
    
    # GhÃ©p láº¡i
    result = " ".join(corrected_chunks)
    
    return result
