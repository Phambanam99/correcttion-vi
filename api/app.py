# -*- coding: utf-8 -*-
"""
Flask API cho Vietnamese Text Corrector
Supports multiple models and pipeline strategies:
- qwen_protonx: Qwen (local) + ProtonX
- qwen_only: Qwen only (local)
- protonx_only: ProtonX only
- bartpho_protonx: BartPho + ProtonX
- ollama_protonx: Ollama (online) + ProtonX
- ollama_only: Ollama only (online)
"""

from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import sys
import os
import queue
import threading
import uuid
from datetime import datetime, timedelta

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import QWEN_MODELS, PIPELINE_STRATEGIES, DEFAULT_PIPELINE, MAX_QUEUE_SIZE, JOB_TIMEOUT_SECONDS, JOB_CLEANUP_HOURS
from llm.bartpho_model import correct_text as bartpho_correct, correct_text_chunked as bartpho_chunked
from llm.qwen_model import correct_text as qwen_correct, get_available_models as get_qwen_models
from protonx_layer.protonx_refine import refine_text_chunked
from processor.diff_utils import generate_change_note, is_meaningful_text

# Load Ollama model
ollama_models_list = []
try:
    from llm.ollama_model import correct_text as ollama_correct, check_ollama_health, get_available_models as get_ollama_models
    ollama_available = check_ollama_health()
    if ollama_available:
        print("‚úÖ Ollama API is reachable")
        ollama_models_list = get_ollama_models()
    else:
        print("‚ö†Ô∏è Ollama API kh√¥ng kh·∫£ d·ª•ng")
except Exception as e:
    print(f"‚ö†Ô∏è Ollama module error: {e}")
    ollama_available = False
    ollama_correct = None

# Load Vistral model (gated model, c·∫ßn HF_TOKEN)
vistral_available = False
vistral_correct = None

try:
    from llm.vistral_model import correct_text as vistral_correct
    vistral_available = True
    print("‚úÖ Vistral model loaded successfully")
except Exception as e:
    print(f"‚ö†Ô∏è Vistral model kh√¥ng kh·∫£ d·ª•ng: {e}")
    vistral_available = False

app = Flask(__name__)
CORS(app)  # Enable CORS for web frontend

# C·∫•u h√¨nh
MAX_WORDS_PER_CHUNK = 100

# Available models: base models + qwen variants (ollama models are fetched dynamically)
AVAILABLE_MODELS = ["bartpho", "qwen", "vistral"] + [f"qwen-{k}" for k in QWEN_MODELS.keys()]
DEFAULT_MODEL = "qwen"

# ===== JOB QUEUE SYSTEM =====
# Job statuses
JOB_STATUS_PENDING = "pending"
JOB_STATUS_PROCESSING = "processing"
JOB_STATUS_COMPLETED = "completed"
JOB_STATUS_FAILED = "failed"

# In-memory job store
job_store = {}  # {job_id: {status, created_at, result, error, ...}}
job_queue = queue.Queue(maxsize=MAX_QUEUE_SIZE)
job_store_lock = threading.Lock()


def job_worker():
    """Background worker thread to process jobs from queue"""
    while True:
        try:
            job_id = job_queue.get(timeout=1)
        except queue.Empty:
            continue
        
        try:
            with job_store_lock:
                if job_id not in job_store:
                    continue
                job = job_store[job_id]
                job["status"] = JOB_STATUS_PROCESSING
                job["started_at"] = datetime.now().isoformat()
            
            # Process the job
            text = job["text"]
            pipeline = job.get("pipeline", DEFAULT_PIPELINE)
            qwen_variant = job.get("qwen_model")
            ollama_model = job.get("ollama_model")
            
            # Execute correction
            final_text, explanation = correct_with_pipeline(
                text, 
                pipeline=pipeline, 
                qwen_variant=qwen_variant, 
                ollama_model=ollama_model
            )
            
            note = generate_change_note(text, final_text)
            
            with job_store_lock:
                job_store[job_id].update({
                    "status": JOB_STATUS_COMPLETED,
                    "completed_at": datetime.now().isoformat(),
                    "result": {
                        "original": text,
                        "corrected": final_text,
                        "explanation": explanation,
                        "note": note or "",
                        "has_changes": text != final_text
                    }
                })
            
            print(f"‚úÖ Job {job_id[:8]}... completed")
            
        except Exception as e:
            import traceback
            with job_store_lock:
                if job_id in job_store:
                    job_store[job_id].update({
                        "status": JOB_STATUS_FAILED,
                        "completed_at": datetime.now().isoformat(),
                        "error": str(e),
                        "traceback": traceback.format_exc()
                    })
            print(f"‚ùå Job {job_id[:8]}... failed: {e}")
        
        finally:
            job_queue.task_done()


def cleanup_old_jobs():
    """Remove completed jobs older than JOB_CLEANUP_HOURS"""
    cutoff = datetime.now() - timedelta(hours=JOB_CLEANUP_HOURS)
    with job_store_lock:
        to_remove = []
        for job_id, job in job_store.items():
            if job["status"] in [JOB_STATUS_COMPLETED, JOB_STATUS_FAILED]:
                created = datetime.fromisoformat(job["created_at"])
                if created < cutoff:
                    to_remove.append(job_id)
        for job_id in to_remove:
            del job_store[job_id]
        if to_remove:
            print(f"üßπ Cleaned up {len(to_remove)} old jobs")


# Start worker thread
worker_thread = threading.Thread(target=job_worker, daemon=True)
worker_thread.start()
print("üîÑ Job worker thread started")


def correct_with_model(text: str, model: str = DEFAULT_MODEL, qwen_variant: str = None) -> tuple:
    """
    S·ª≠a l·ªói vƒÉn b·∫£n v·ªõi model ƒë∆∞·ª£c ch·ªçn.
    Returns: (corrected_text, explanation)
    
    Args:
        text: VƒÉn b·∫£n c·∫ßn s·ª≠a
        model: Model ch√≠nh (bartpho, qwen, vistral, ho·∫∑c qwen-<variant>)
        qwen_variant: Variant c·ªßa Qwen model (qwen2.5-7b, qwen3-8b)
    """
    word_count = len(text.split())
    
    # Handle qwen-<variant> format
    if model.startswith("qwen-"):
        qwen_variant = model.replace("qwen-", "")
        model = "qwen"
    
    if model == "qwen":
        # Qwen tr·∫£ v·ªÅ tuple (text, explanation)
        corrected, explanation = qwen_correct(text, model_key=qwen_variant)
        return corrected, explanation
    elif model == "vistral":
        # Vistral model
        if vistral_available and vistral_correct:
            corrected, explanation = vistral_correct(text)
            return corrected, explanation
        else:
            # Fallback to Qwen n·∫øu Vistral kh√¥ng available
            print("‚ö†Ô∏è Vistral kh√¥ng kh·∫£ d·ª•ng, d√πng Qwen thay th·∫ø")
            corrected, explanation = qwen_correct(text)
            explanation = "‚ö†Ô∏è Vistral kh√¥ng kh·∫£ d·ª•ng (c·∫ßn HF_TOKEN). ƒê√£ d√πng Qwen."
            return corrected, explanation
    else:
        # BartPho (default)
        if word_count > MAX_WORDS_PER_CHUNK:
            corrected = bartpho_chunked(text, MAX_WORDS_PER_CHUNK)
        else:
            corrected = bartpho_correct(text)
        explanation = generate_explanation(text, corrected)
        return corrected, explanation


def correct_with_pipeline(text: str, model: str = DEFAULT_MODEL, pipeline: str = DEFAULT_PIPELINE, qwen_variant: str = None, ollama_model: str = None) -> tuple:
    """
    S·ª≠a l·ªói vƒÉn b·∫£n v·ªõi pipeline ƒë∆∞·ª£c ch·ªçn.
    Returns: (corrected_text, explanation)
    
    Pipeline strategies:
    - qwen_protonx: Qwen ‚Üí ProtonX refine
    - qwen_only: Ch·ªâ Qwen
    - protonx_only: Ch·ªâ ProtonX
    - bartpho_protonx: BartPho ‚Üí ProtonX refine
    - ollama_protonx: Ollama ‚Üí ProtonX refine (online)
    - ollama_only: Ch·ªâ Ollama (online)
    """
    word_count = len(text.split())
    
    if pipeline == "qwen_only":
        # Ch·ªâ d√πng Qwen, kh√¥ng ProtonX
        corrected, explanation = qwen_correct(text, model_key=qwen_variant)
        return corrected, explanation
    
    elif pipeline == "protonx_only":
        # Ch·ªâ d√πng ProtonX
        corrected = refine_text_chunked(text, MAX_WORDS_PER_CHUNK)
        explanation = "ƒê√£ refine v·ªõi ProtonX (kh√¥ng qua LLM)"
        return corrected, explanation
    
    elif pipeline == "bartpho_protonx":
        # BartPho + ProtonX
        if word_count > MAX_WORDS_PER_CHUNK:
            model_fixed = bartpho_chunked(text, MAX_WORDS_PER_CHUNK)
        else:
            model_fixed = bartpho_correct(text)
        # ProtonX refine
        final_text = refine_text_chunked(model_fixed, MAX_WORDS_PER_CHUNK)
        explanation = generate_explanation(text, final_text)
        return final_text, explanation
    
    elif pipeline == "ollama_only":
        # Ch·ªâ d√πng Ollama (online), kh√¥ng ProtonX
        if ollama_available and ollama_correct:
            corrected, explanation = ollama_correct(text, model_key=ollama_model)
            return corrected, explanation
        else:
            # Fallback to Qwen
            print("‚ö†Ô∏è Ollama kh√¥ng kh·∫£ d·ª•ng, d√πng Qwen thay th·∫ø")
            corrected, explanation = qwen_correct(text, model_key=qwen_variant)
            explanation = "‚ö†Ô∏è Ollama API kh√¥ng kh·∫£ d·ª•ng. ƒê√£ d√πng Qwen local."
            return corrected, explanation
    
    elif pipeline == "ollama_protonx":
        # Ollama (online) + ProtonX
        if ollama_available and ollama_correct:
            model_fixed, explanation = ollama_correct(text, model_key=ollama_model)
        else:
            print("‚ö†Ô∏è Ollama kh√¥ng kh·∫£ d·ª•ng, d√πng Qwen thay th·∫ø")
            model_fixed, explanation = qwen_correct(text, model_key=qwen_variant)
            explanation = "‚ö†Ô∏è Ollama API kh√¥ng kh·∫£ d·ª•ng. ƒê√£ d√πng Qwen local."
        # ProtonX refine
        final_text = refine_text_chunked(model_fixed, MAX_WORDS_PER_CHUNK)
        return final_text, explanation
    
    else:  # qwen_protonx (default)
        # Qwen + ProtonX
        model_fixed, explanation = qwen_correct(text, model_key=qwen_variant)
        # ProtonX refine
        final_text = refine_text_chunked(model_fixed, MAX_WORDS_PER_CHUNK)
        return final_text, explanation


def generate_explanation(original: str, corrected: str) -> str:
    """T·∫°o gi·∫£i th√≠ch ng·∫Øn g·ªçn v·ªÅ c√°c thay ƒë·ªïi"""
    if original.strip() == corrected.strip():
        return "Kh√¥ng c√≥ thay ƒë·ªïi."
    
    original_words = set(original.lower().split())
    corrected_words = set(corrected.lower().split())
    
    added = corrected_words - original_words
    removed = original_words - corrected_words
    
    explanations = []
    if removed:
        explanations.append(f"S·ª≠a: {', '.join(list(removed)[:5])}")
    if added:
        explanations.append(f"Th√†nh: {', '.join(list(added)[:5])}")
    
    return " ‚Üí ".join(explanations) if explanations else "ƒê√£ s·ª≠a d·∫•u v√† ƒë·ªãnh d·∫°ng."


@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        "status": "ok",
        "message": "Vietnamese Text Corrector API is running",
        "available_models": AVAILABLE_MODELS,
        "qwen_models": list(QWEN_MODELS.keys()),
        "ollama_models": ollama_models_list,
        "ollama_available": ollama_available,
        "available_pipelines": PIPELINE_STRATEGIES,
        "default_model": DEFAULT_MODEL,
        "default_pipeline": DEFAULT_PIPELINE
    })


@app.route('/api/ollama-models', methods=['GET'])
def get_ollama_models_endpoint():
    """
    Get available Ollama models from the remote API
    
    Response:
    {
        "success": true,
        "available": true/false,
        "models": ["model1", "model2", ...]
    }
    """
    global ollama_models_list
    
    if not ollama_available:
        return jsonify({
            "success": True,
            "available": False,
            "models": [],
            "message": "Ollama API kh√¥ng kh·∫£ d·ª•ng"
        })
    
    try:
        # Refresh models from API
        models = get_ollama_models()
        ollama_models_list = models
        
        return jsonify({
            "success": True,
            "available": True,
            "models": models
        })
    except Exception as e:
        return jsonify({
            "success": False,
            "available": False,
            "models": ollama_models_list,
            "error": str(e)
        })


@app.route('/api/correct', methods=['POST'])
def correct_text():
    """
    S·ª≠a l·ªói vƒÉn b·∫£n
    
    Request body:
    {
        "text": "vƒÉn b·∫£n c·∫ßn s·ª≠a",
        "pipeline": "qwen_protonx" (optional)
    }
    """
    try:
        data = request.get_json()
        if not data or 'text' not in data:
            return jsonify({
                "success": False,
                "error": "Missing 'text' field in request body"
            }), 400
        
        original = data['text'].strip()
        if not original:
            return jsonify({
                "success": False,
                "error": "Text cannot be empty"
            }), 400
        
        # L·∫•y pipeline
        pipeline = data.get('pipeline', DEFAULT_PIPELINE)
        if pipeline not in PIPELINE_STRATEGIES:
            pipeline = DEFAULT_PIPELINE
        
        qwen_variant = data.get('qwen_model', None)
        
        # S·ª≠a l·ªói v·ªõi pipeline
        final_text, explanation = correct_with_pipeline(original, pipeline=pipeline, qwen_variant=qwen_variant)
        
        # T·∫°o ghi ch√∫ thay ƒë·ªïi
        note = generate_change_note(original, final_text)
        
        return jsonify({
            "success": True,
            "original": original,
            "corrected": final_text,
            "explanation": explanation,
            "pipeline_used": pipeline,
            "note": note or ""
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }), 500


@app.route('/api/submit-job', methods=['POST'])
def submit_job():
    """
    Submit a text correction job to the queue (async processing).
    Returns immediately with a job ID that can be polled for status.
    
    Request body:
    {
        "text": "vƒÉn b·∫£n c·∫ßn s·ª≠a",
        "pipeline": "qwen_protonx" (optional),
        "qwen_model": "qwen3-8b" (optional),
        "ollama_model": "qwen2.5:7b" (optional)
    }
    
    Response:
    {
        "success": true,
        "job_id": "uuid-string",
        "queue_position": 5,
        "message": "Job submitted successfully"
    }
    """
    try:
        data = request.get_json()
        if not data or 'text' not in data:
            return jsonify({
                "success": False,
                "error": "Missing 'text' field in request body"
            }), 400
        
        text = data['text'].strip()
        if not text:
            return jsonify({
                "success": False,
                "error": "Text cannot be empty"
            }), 400
        
        # Check if queue is full
        if job_queue.full():
            return jsonify({
                "success": False,
                "error": "Queue is full. Please try again later.",
                "queue_size": job_queue.qsize()
            }), 503
        
        # Create job
        job_id = str(uuid.uuid4())
        pipeline = data.get('pipeline', DEFAULT_PIPELINE)
        if pipeline not in PIPELINE_STRATEGIES:
            pipeline = DEFAULT_PIPELINE
        
        job = {
            "job_id": job_id,
            "text": text,
            "pipeline": pipeline,
            "qwen_model": data.get('qwen_model'),
            "ollama_model": data.get('ollama_model'),
            "status": JOB_STATUS_PENDING,
            "created_at": datetime.now().isoformat(),
            "result": None,
            "error": None
        }
        
        with job_store_lock:
            job_store[job_id] = job
        
        job_queue.put(job_id)
        
        # Cleanup old jobs periodically
        if len(job_store) > MAX_QUEUE_SIZE * 2:
            cleanup_old_jobs()
        
        return jsonify({
            "success": True,
            "job_id": job_id,
            "queue_position": job_queue.qsize(),
            "message": "Job submitted successfully"
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }), 500


@app.route('/api/job-status/<job_id>', methods=['GET'])
def get_job_status(job_id):
    """
    Get the status of a submitted job.
    
    Response when pending:
    {
        "success": true,
        "status": "pending",
        "queue_position": 3
    }
    
    Response when completed:
    {
        "success": true,
        "status": "completed",
        "result": {
            "original": "...",
            "corrected": "...",
            "explanation": "...",
            "has_changes": true
        }
    }
    """
    with job_store_lock:
        if job_id not in job_store:
            return jsonify({
                "success": False,
                "error": "Job not found"
            }), 404
        
        job = job_store[job_id].copy()
    
    response = {
        "success": True,
        "job_id": job_id,
        "status": job["status"],
        "created_at": job["created_at"]
    }
    
    if job["status"] == JOB_STATUS_PENDING:
        response["queue_position"] = job_queue.qsize()
    elif job["status"] == JOB_STATUS_PROCESSING:
        response["started_at"] = job.get("started_at")
    elif job["status"] == JOB_STATUS_COMPLETED:
        response["completed_at"] = job.get("completed_at")
        response["result"] = job.get("result")
    elif job["status"] == JOB_STATUS_FAILED:
        response["completed_at"] = job.get("completed_at")
        response["error"] = job.get("error")
    
    return jsonify(response)


@app.route('/api/queue-status', methods=['GET'])
def get_queue_status():
    """
    Get the current queue status.
    
    Response:
    {
        "success": true,
        "queue_size": 5,
        "max_queue_size": 50,
        "pending_jobs": 3,
        "processing_jobs": 1,
        "completed_jobs": 10
    }
    """
    with job_store_lock:
        pending = sum(1 for j in job_store.values() if j["status"] == JOB_STATUS_PENDING)
        processing = sum(1 for j in job_store.values() if j["status"] == JOB_STATUS_PROCESSING)
        completed = sum(1 for j in job_store.values() if j["status"] == JOB_STATUS_COMPLETED)
        failed = sum(1 for j in job_store.values() if j["status"] == JOB_STATUS_FAILED)
    
    return jsonify({
        "success": True,
        "queue_size": job_queue.qsize(),
        "max_queue_size": MAX_QUEUE_SIZE,
        "pending_jobs": pending,
        "processing_jobs": processing,
        "completed_jobs": completed,
        "failed_jobs": failed,
        "total_jobs": len(job_store)
    })


@app.route('/api/correct-paragraphs', methods=['POST'])
def correct_paragraphs():
    """
    S·ª≠a l·ªói nhi·ªÅu ƒëo·∫°n vƒÉn (t√°ch b·∫±ng newline)
    
    Request body:
    {
        "text": "ƒëo·∫°n 1\nƒëo·∫°n 2\nƒëo·∫°n 3",
        "model": "qwen" ho·∫∑c "bartpho" (m·∫∑c ƒë·ªãnh: qwen),
        "pipeline": "qwen_protonx" ho·∫∑c "qwen_only" ho·∫∑c "protonx_only" ho·∫∑c "bartpho_protonx",
        "qwen_model": "qwen2.5-7b" ho·∫∑c "qwen3-8b" (optional)
    }
    """
    try:
        data = request.get_json()
        if not data or 'text' not in data:
            return jsonify({
                "success": False,
                "error": "Missing 'text' field in request body"
            }), 400
        
        text = data['text'].strip()
        if not text:
            return jsonify({
                "success": False,
                "error": "Text cannot be empty"
            }), 400
        
        # L·∫•y model v√† pipeline
        model = data.get('model', DEFAULT_MODEL).lower()
        pipeline = data.get('pipeline', DEFAULT_PIPELINE)
        qwen_variant = data.get('qwen_model', None)
        ollama_model_name = None
        
        # Handle ollama-<model> format
        if model.startswith("ollama-"):
            ollama_model_name = model.replace("ollama-", "")
            # Auto-switch to ollama pipeline if using ollama model
            if pipeline not in ["ollama_only", "ollama_protonx"]:
                pipeline = "ollama_protonx"  # Default to ollama + protonx
            model = "ollama"
        
        # Handle qwen-<variant> format
        elif model.startswith("qwen-"):
            qwen_variant = model.replace("qwen-", "")
            model = "qwen"
        
        # Validate pipeline
        if pipeline not in PIPELINE_STRATEGIES:
            pipeline = DEFAULT_PIPELINE
        
        # Chia th√†nh c√°c ƒëo·∫°n
        paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
        
        results = []
        corrected_paragraphs = []
        
        for i, original in enumerate(paragraphs):
            # Ki·ªÉm tra ƒëo·∫°n vƒÉn c√≥ √Ω nghƒ©a ƒë·ªÉ x·ª≠ l√Ω hay kh√¥ng
            if not is_meaningful_text(original):
                # B·ªè qua ƒëo·∫°n kh√¥ng c√≥ √Ω nghƒ©a, gi·ªØ nguy√™n
                results.append({
                    "index": i,
                    "original": original,
                    "corrected": original,
                    "explanation": "ƒêo·∫°n vƒÉn kh√¥ng c√≥ n·ªôi dung √Ω nghƒ©a ƒë·ªÉ x·ª≠ l√Ω",
                    "note": "",
                    "has_changes": False,
                    "skipped": True
                })
                corrected_paragraphs.append(original)
                continue
            
            # S·ª≠a l·ªói v·ªõi pipeline
            final_text, explanation = correct_with_pipeline(original, model=model, pipeline=pipeline, qwen_variant=qwen_variant, ollama_model=ollama_model_name)
            
            note = generate_change_note(original, final_text)
            
            results.append({
                "index": i,
                "original": original,
                "corrected": final_text,
                "explanation": explanation,
                "note": note or "",
                "has_changes": original != final_text
            })
            
            corrected_paragraphs.append(final_text)
        
        return jsonify({
            "success": True,
            "model_used": model,
            "pipeline_used": pipeline,
            "qwen_model_used": qwen_variant,
            "ollama_model_used": ollama_model_name,
            "total_paragraphs": len(paragraphs),
            "results": results,
            "full_corrected": '\n\n'.join(corrected_paragraphs)
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }), 500


@app.route('/api/upload-docx', methods=['POST'])
def upload_docx():
    """
    Upload v√† ƒë·ªçc n·ªôi dung file DOCX
    """
    try:
        if 'file' not in request.files:
            return jsonify({
                "success": False,
                "error": "No file uploaded"
            }), 400
        
        file = request.files['file']
        
        if file.filename == '':
            return jsonify({
                "success": False,
                "error": "No file selected"
            }), 400
        
        if not file.filename.endswith('.docx'):
            return jsonify({
                "success": False,
                "error": "Only .docx files are supported"
            }), 400
        
        # ƒê·ªçc file DOCX
        from docx import Document
        import io
        
        doc = Document(io.BytesIO(file.read()))
        
        # L·∫•y text t·ª´ t·∫•t c·∫£ paragraphs
        paragraphs = [para.text for para in doc.paragraphs if para.text.strip()]
        text = '\n'.join(paragraphs)
        
        return jsonify({
            "success": True,
            "filename": file.filename,
            "text": text,
            "paragraph_count": len(paragraphs)
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }), 500


@app.route('/api/download-docx', methods=['POST'])
def download_docx():
    """
    T·∫°o file DOCX t·ª´ vƒÉn b·∫£n
    """
    try:
        data = request.get_json()
        if not data or 'text' not in data:
            return jsonify({
                "success": False,
                "error": "Missing 'text' field"
            }), 400
        
        text = data['text'].strip()
        filename = data.get('filename', 'corrected_output.docx')
        
        if not text:
            return jsonify({
                "success": False,
                "error": "Text cannot be empty"
            }), 400
        
        # T·∫°o file DOCX
        from docx import Document
        from flask import send_file
        import io
        
        doc = Document()
        
        # Th√™m c√°c ƒëo·∫°n vƒÉn
        for para in text.split('\n\n'):
            if para.strip():
                doc.add_paragraph(para.strip())
        
        # L∆∞u v√†o memory buffer
        buffer = io.BytesIO()
        doc.save(buffer)
        buffer.seek(0)
        
        return send_file(
            buffer,
            as_attachment=True,
            download_name=filename,
            mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        )
        
    except Exception as e:
        import traceback
        return jsonify({
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }), 500


@app.route('/api/correct-docx', methods=['POST'])
def correct_docx():
    """
    Upload DOCX, s·ª≠a l·ªói, v√† tr·∫£ v·ªÅ DOCX v·ªõi comments ghi ch√∫ thay ƒë·ªïi.
    """
    try:
        from docx import Document
        from docx.shared import Pt, RGBColor
        import io
        
        if 'file' not in request.files:
            return jsonify({
                "success": False,
                "error": "No file uploaded"
            }), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({
                "success": False,
                "error": "No file selected"
            }), 400
        
        if not file.filename.endswith('.docx'):
            return jsonify({
                "success": False,
                "error": "Only .docx files are supported"
            }), 400
        
        # L·∫•y model v√† pipeline t·ª´ form data
        model = request.form.get('model', DEFAULT_MODEL).lower()
        pipeline = request.form.get('pipeline', DEFAULT_PIPELINE)
        qwen_variant = request.form.get('qwen_model', None)
        
        if model not in AVAILABLE_MODELS:
            model = DEFAULT_MODEL
        if pipeline not in PIPELINE_STRATEGIES:
            pipeline = DEFAULT_PIPELINE
        
        # ƒê·ªçc file DOCX
        doc = Document(io.BytesIO(file.read()))
        
        # T·∫°o document m·ªõi v·ªõi n·ªôi dung ƒë√£ s·ª≠a
        new_doc = Document()
        changes_log = []
        
        for para_idx, para in enumerate(doc.paragraphs):
            original_text = para.text.strip()
            
            if not original_text:
                new_doc.add_paragraph()
                continue
            
            # Ki·ªÉm tra ƒëo·∫°n vƒÉn c√≥ √Ω nghƒ©a ƒë·ªÉ x·ª≠ l√Ω hay kh√¥ng
            if not is_meaningful_text(original_text):
                # B·ªè qua ƒëo·∫°n kh√¥ng c√≥ √Ω nghƒ©a, gi·ªØ nguy√™n
                new_doc.add_paragraph(original_text)
                continue
            
            # S·ª≠a l·ªói v·ªõi pipeline
            final_text, explanation = correct_with_pipeline(original_text, model=model, pipeline=pipeline, qwen_variant=qwen_variant)
            
            # Th√™m paragraph ƒë√£ s·ª≠a
            new_para = new_doc.add_paragraph(final_text)
            
            # N·∫øu c√≥ thay ƒë·ªïi, ghi ch√∫
            if original_text != final_text:
                changes_log.append({
                    "paragraph": para_idx + 1,
                    "original": original_text,
                    "corrected": final_text,
                    "explanation": explanation
                })
        
        # Th√™m ph·∫ßn t·ªïng k·∫øt thay ƒë·ªïi ·ªü cu·ªëi
        if changes_log:
            new_doc.add_paragraph()
            summary_para = new_doc.add_paragraph()
            summary_run = summary_para.add_run("‚ïê‚ïê‚ïê T·ªîNG K·∫æT C√ÅC THAY ƒê·ªîI ‚ïê‚ïê‚ïê")
            summary_run.bold = True
            summary_run.font.size = Pt(14)
            summary_run.font.color.rgb = RGBColor(0, 102, 204)
            
            for change in changes_log:
                new_doc.add_paragraph()
                
                # Ti√™u ƒë·ªÅ ƒëo·∫°n
                title_para = new_doc.add_paragraph()
                title_run = title_para.add_run(f"üìç ƒêo·∫°n {change['paragraph']}:")
                title_run.bold = True
                
                # VƒÉn b·∫£n g·ªëc
                orig_para = new_doc.add_paragraph()
                orig_run = orig_para.add_run("‚ùå G·ªëc: ")
                orig_run.font.color.rgb = RGBColor(204, 0, 0)
                orig_para.add_run(change['original'][:200] + "..." if len(change['original']) > 200 else change['original'])
                
                # VƒÉn b·∫£n ƒë√£ s·ª≠a
                corr_para = new_doc.add_paragraph()
                corr_run = corr_para.add_run("‚úÖ S·ª≠a: ")
                corr_run.font.color.rgb = RGBColor(0, 153, 0)
                corr_para.add_run(change['corrected'][:200] + "..." if len(change['corrected']) > 200 else change['corrected'])
                
                # Gi·∫£i th√≠ch
                if change['explanation']:
                    exp_para = new_doc.add_paragraph()
                    exp_run = exp_para.add_run("üí¨ Ch√∫ th√≠ch: ")
                    exp_run.italic = True
                    exp_para.add_run(change['explanation'])
        
        # L∆∞u v√†o buffer
        buffer = io.BytesIO()
        new_doc.save(buffer)
        buffer.seek(0)
        
        # T·∫°o t√™n file output
        output_filename = file.filename.replace('.docx', '_corrected.docx')
        
        return send_file(
            buffer,
            as_attachment=True,
            download_name=output_filename,
            mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        )
        
    except Exception as e:
        import traceback
        return jsonify({
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }), 500


if __name__ == '__main__':
    print("üöÄ Starting Vietnamese Text Corrector API...")
    print("üìç API running at: http://localhost:5000")
    print("üìñ Endpoints:")
    print("   GET  /api/health - Health check (shows available models & pipelines)")
    print("   POST /api/correct - Correct single text (sync)")
    print("   POST /api/correct-paragraphs - Correct multiple paragraphs (sync)")
    print("   POST /api/submit-job - Submit job to queue (async)")
    print("   GET  /api/job-status/<id> - Get job status/result")
    print("   GET  /api/queue-status - Get queue statistics")
    print("   POST /api/upload-docx - Upload DOCX file")
    print("   POST /api/download-docx - Download as DOCX")
    print("   POST /api/correct-docx - Upload & correct DOCX with comments")
    print("=" * 50)
    print(f"ü§ñ Available models: {AVAILABLE_MODELS}")
    print(f"üîß Available pipelines: {PIPELINE_STRATEGIES}")
    print(f"üìä Max queue size: {MAX_QUEUE_SIZE}")
    print("=" * 50)
    
    # Enable threaded mode for concurrent request handling
    app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)
